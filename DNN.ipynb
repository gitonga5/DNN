{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e85d0415",
   "metadata": {},
   "source": [
    "##### 【Problem 1 】 Classification of total join layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df4e6b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class FC:\n",
    "    \"\"\"\n",
    "    Fully connected layer from n_nodes1 to n_nodes2.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      Number of nodes in the previous layer.\n",
    "    n_nodes2 : int\n",
    "      Number of nodes in the subsequent layer.\n",
    "    initializer : Initialization method instance\n",
    "    optimizer : Optimization method instance\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        # Initialize weights and biases using the initializer.\n",
    "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
    "        self.B = initializer.B(n_nodes2)\n",
    "        # Store the input during the forward pass.\n",
    "        self.X = None\n",
    "        # Store the gradients calculated during the backward pass.\n",
    "        self.dW = None\n",
    "        self.dB = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of shape (batch_size, n_nodes1)\n",
    "            Input from the previous layer.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        A : ndarray of shape (batch_size, n_nodes2)\n",
    "            Output of this layer before activation.\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        A = np.dot(X, self.W) + self.B\n",
    "        return A\n",
    "\n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Backward propagation.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : ndarray of shape (batch_size, n_nodes2)\n",
    "            Gradient from the subsequent layer.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : ndarray of shape (batch_size, n_nodes1)\n",
    "            Gradient to the previous layer.\n",
    "        \"\"\"\n",
    "        # Calculate the gradient of the loss with respect to the weights.\n",
    "        self.dW = np.dot(self.X.T, dA)\n",
    "        # Calculate the gradient of the loss with respect to the biases.\n",
    "        self.dB = np.sum(dA, axis=0)\n",
    "        # Calculate the gradient to propagate to the previous layer.\n",
    "        dZ = np.dot(dA, self.W.T)\n",
    "        # Update the layer's parameters using the optimizer.\n",
    "        self.optimizer.update(self)\n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673b5210",
   "metadata": {},
   "source": [
    "##### 【Problem 2 】 Initialization method classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c920d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    A simple initializer using a Gaussian distribution.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      Standard deviation of the Gaussian distribution.\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        Initializes weights with a Gaussian distribution.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          Number of nodes in the previous layer.\n",
    "        n_nodes2 : int\n",
    "          Number of nodes in the subsequent layer.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W : ndarray of shape (n_nodes1, n_nodes2)\n",
    "            Initialized weight matrix.\n",
    "        \"\"\"\n",
    "        W = np.random.normal(0, self.sigma, (n_nodes1, n_nodes2))\n",
    "        return W\n",
    "\n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        Initializes biases with zeros.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          Number of nodes in the subsequent layer.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B : ndarray of shape (n_nodes2,)\n",
    "            Initialized bias vector.\n",
    "        \"\"\"\n",
    "        B = np.zeros(n_nodes2)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63ea6a5",
   "metadata": {},
   "source": [
    "##### 【Problem 3 】 Optimization method classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73bdb5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    Stochastic Gradient Descent optimizer.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : float\n",
    "      Learning rate.\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        Updates the weights and biases of a given layer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : instance of the layer to be updated\n",
    "            The layer instance should have attributes `W`, `B`, `dW`, and `dB`.\n",
    "        \"\"\"\n",
    "        layer.W -= self.lr * layer.dW\n",
    "        layer.B -= self.lr * layer.dB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55504626",
   "metadata": {},
   "source": [
    "##### 【Problem 4 】 Activation function classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1a800e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ReLU:\n",
    "    \"\"\"\n",
    "    Rectified Linear Unit activation function.\n",
    "    \"\"\"\n",
    "    def forward(self, A):\n",
    "        \"\"\"\n",
    "        Forward propagation for the ReLU function.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        A : ndarray of shape (batch_size, n_nodes)\n",
    "            Input to the activation function.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        Z : ndarray of shape (batch_size, n_nodes)\n",
    "            Output of the activation function.\n",
    "        \"\"\"\n",
    "        self.mask = (A > 0)\n",
    "        Z = A * self.mask\n",
    "        return Z\n",
    "\n",
    "    def backward(self, dZ):\n",
    "        \"\"\"\n",
    "        Backward propagation for the ReLU function.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dZ : ndarray of shape (batch_size, n_nodes)\n",
    "            Gradient from the subsequent layer.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        dA : ndarray of shape (batch_size, n_nodes)\n",
    "            Gradient to the previous layer.\n",
    "        \"\"\"\n",
    "        dA = dZ * self.mask\n",
    "        return dA\n",
    "\n",
    "class Softmax:\n",
    "    \"\"\"\n",
    "    Softmax activation function.\n",
    "    \"\"\"\n",
    "    def forward(self, A):\n",
    "        \"\"\"\n",
    "        Forward propagation for the softmax function.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        A : ndarray of shape (batch_size, n_nodes)\n",
    "            Input to the activation function.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        Z : ndarray of shape (batch_size, n_nodes)\n",
    "            Output of the activation function (probabilities).\n",
    "        \"\"\"\n",
    "        exp_A = np.exp(A - np.max(A, axis=1, keepdims=True))\n",
    "        self.Z = exp_A / np.sum(exp_A, axis=1, keepdims=True)\n",
    "        return self.Z\n",
    "\n",
    "    def backward(self, Z, y):\n",
    "        \"\"\"\n",
    "        Backward propagation for the softmax function combined with cross-entropy error.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Z : ndarray of shape (batch_size, n_nodes)\n",
    "            Output of the softmax function (probabilities).\n",
    "        y : ndarray of shape (batch_size,)\n",
    "            True labels (integers).\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        dA : ndarray of shape (batch_size, n_nodes)\n",
    "            Gradient to the previous layer.\n",
    "        \"\"\"\n",
    "        batch_size = y.shape[0]\n",
    "        dA = Z - np.eye(Z.shape[1])[y] / batch_size\n",
    "        return dA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400c20ec",
   "metadata": {},
   "source": [
    "##### 【Problem 5 】 ReLU class creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bffc4617",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ReLU:\n",
    "    \"\"\"\n",
    "    Rectified Linear Unit activation function.\n",
    "\n",
    "    f(x) = {x if x > 0, 0 if x <= 0}\n",
    "    \"\"\"\n",
    "    def forward(self, A):\n",
    "        \"\"\"\n",
    "        Forward propagation for the ReLU function.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        A : ndarray of shape (batch_size, n_nodes)\n",
    "            Input to the activation function.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        Z : ndarray of shape (batch_size, n_nodes)\n",
    "            Output of the activation function.\n",
    "        \"\"\"\n",
    "        self.mask = (A > 0)\n",
    "        Z = np.maximum(0, A)\n",
    "        return Z\n",
    "\n",
    "    def backward(self, dZ):\n",
    "        \"\"\"\n",
    "        Backward propagation for the ReLU function.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dZ : ndarray of shape (batch_size, n_nodes)\n",
    "            Gradient from the subsequent layer.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        dA : ndarray of shape (batch_size, n_nodes)\n",
    "            Gradient to the previous layer.\n",
    "        \"\"\"\n",
    "        dA = dZ * self.mask\n",
    "        return dA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284ad537",
   "metadata": {},
   "source": [
    "##### 【Problem 6 】 Initial weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b90bba3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class XavierInitializer:\n",
    "    \"\"\"\n",
    "    Xavier initializer (also known as Glorot initializer).\n",
    "\n",
    "    The standard deviation is calculated as sigma = 1 / sqrt(n_nodes1),\n",
    "    where n_nodes1 is the number of nodes in the previous layer.\n",
    "    \"\"\"\n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        Initializes weights with a normal distribution based on the Xavier method.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          Number of nodes in the previous layer.\n",
    "        n_nodes2 : int\n",
    "          Number of nodes in the subsequent layer.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W : ndarray of shape (n_nodes1, n_nodes2)\n",
    "            Initialized weight matrix.\n",
    "        \"\"\"\n",
    "        sigma = 1.0 / np.sqrt(n_nodes1)\n",
    "        W = np.random.normal(0, sigma, (n_nodes1, n_nodes2))\n",
    "        return W\n",
    "\n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        Initializes biases with zeros.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          Number of nodes in the subsequent layer.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B : ndarray of shape (n_nodes2,)\n",
    "            Initialized bias vector.\n",
    "        \"\"\"\n",
    "        B = np.zeros(n_nodes2)\n",
    "        return B\n",
    "\n",
    "class HeInitializer:\n",
    "    \"\"\"\n",
    "    He initializer.\n",
    "\n",
    "    The standard deviation is calculated as sigma = sqrt(2 / n_nodes1),\n",
    "    where n_nodes1 is the number of nodes in the previous layer.\n",
    "    \"\"\"\n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        Initializes weights with a normal distribution based on the He method.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          Number of nodes in the previous layer.\n",
    "        n_nodes2 : int\n",
    "          Number of nodes in the subsequent layer.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W : ndarray of shape (n_nodes1, n_nodes2)\n",
    "            Initialized weight matrix.\n",
    "        \"\"\"\n",
    "        sigma = np.sqrt(2.0 / n_nodes1)\n",
    "        W = np.random.normal(0, sigma, (n_nodes1, n_nodes2))\n",
    "        return W\n",
    "\n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        Initializes biases with zeros.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          Number of nodes in the subsequent layer.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B : ndarray of shape (n_nodes2,)\n",
    "            Initialized bias vector.\n",
    "        \"\"\"\n",
    "        B = np.zeros(n_nodes2)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7972f897",
   "metadata": {},
   "source": [
    "##### 【Problem 7 】 Optimization method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "726295ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SGD:\n",
    "    \"\"\"\n",
    "    Stochastic Gradient Descent optimizer.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : float\n",
    "      Learning rate.\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        Updates the weights and biases of a given layer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : instance of the layer to be updated\n",
    "            The layer instance should have attributes `W`, `B`, `dW`, and `dB`.\n",
    "        \"\"\"\n",
    "        layer.W -= self.lr * layer.dW\n",
    "        layer.B -= self.lr * layer.dB\n",
    "\n",
    "class AdaGrad:\n",
    "    \"\"\"\n",
    "    AdaGrad optimizer.\n",
    "\n",
    "    Adapts the learning rate for each parameter based on the historical\n",
    "    gradient magnitudes.\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        self.hw = None  # Accumulated squared gradients for weights\n",
    "        self.hb = None  # Accumulated squared gradients for biases\n",
    "        self.epsilon = 1e-7  # Small constant to avoid division by zero\n",
    "\n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        Updates the weights and biases of a given layer using AdaGrad.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : instance of the layer to be updated\n",
    "            The layer instance should have attributes `W`, `B`, `dW`, and `dB`.\n",
    "        \"\"\"\n",
    "        if self.hw is None:\n",
    "            self.hw = np.zeros_like(layer.W)\n",
    "            self.hb = np.zeros_like(layer.B)\n",
    "\n",
    "        self.hw += layer.dW * layer.dW\n",
    "        self.hb += layer.dB * layer.dB\n",
    "\n",
    "        layer.W -= self.lr * (1 / (np.sqrt(self.hw) + self.epsilon)) * layer.dW\n",
    "        layer.B -= self.lr * (1 / (np.sqrt(self.hb) + self.epsilon)) * layer.dB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96a47a0",
   "metadata": {},
   "source": [
    "##### 【Problem 8 】 Class completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ca03404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions (first 10): [4 4 4 5 4 4 5 4 4 2]\n",
      "Probabilities (first 2 samples):\n",
      " [[0.07877131 0.03817066 0.13181438 0.14385221 0.20104698 0.13713206\n",
      "  0.07302206 0.06763242 0.0784162  0.05014172]\n",
      " [0.07066045 0.06668568 0.07441456 0.09451205 0.21325199 0.16557688\n",
      "  0.09807867 0.10509788 0.06783265 0.04388918]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class ScratchDeepNeuralNetrowkClassifier:\n",
    "    \"\"\"\n",
    "    A versatile deep neural network classifier.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_layers, layer_configs, n_features, n_output, random_state=None):\n",
    "        self.n_layers = n_layers\n",
    "        self.layer_configs = layer_configs\n",
    "        self.n_features = n_features\n",
    "        self.n_output = n_output\n",
    "        self.layers = []\n",
    "        self.activations = []\n",
    "        self.loss_history = []\n",
    "        self.random_state = random_state\n",
    "        if self.random_state is not None:\n",
    "            np.random.seed(self.random_state)\n",
    "        self._build_network()\n",
    "\n",
    "    def _build_network(self):\n",
    "        input_dim = self.n_features\n",
    "        for i, config in enumerate(self.layer_configs):\n",
    "            layer_type = config['type']\n",
    "            n_nodes = config['n_nodes']\n",
    "            activation_type = config['activation']\n",
    "            initializer = config['initializer']\n",
    "            optimizer = config['optimizer']\n",
    "\n",
    "            if layer_type == 'FC':\n",
    "                fc_layer = FC(input_dim, n_nodes, initializer, optimizer)\n",
    "                self.layers.append(fc_layer)\n",
    "                if activation_type == 'Tanh':\n",
    "                    self.activations.append(Tanh())\n",
    "                elif activation_type == 'Softmax':\n",
    "                    self.activations.append(Softmax())\n",
    "                elif activation_type == 'ReLU':\n",
    "                    self.activations.append(ReLU())\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown activation type: {activation_type}\")\n",
    "                input_dim = n_nodes\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown layer type: {layer_type}\")\n",
    "\n",
    "        # Ensure the last layer has n_output nodes and Softmax activation\n",
    "        if not self.layer_configs or self.layer_configs[-1]['n_nodes'] != self.n_output or self.layer_configs[-1]['activation'] != 'Softmax':\n",
    "            last_input_dim = self.layer_configs[-1]['n_nodes'] if self.layer_configs else self.n_features\n",
    "            output_config = {'type': 'FC', 'n_nodes': self.n_output, 'activation': 'Softmax', 'initializer': XavierInitializer(), 'optimizer': AdaGrad(self.layer_configs[-1]['optimizer'].lr if self.layer_configs else 0.01)}\n",
    "            output_fc = FC(last_input_dim, self.n_output, output_config['initializer'], output_config['optimizer'])\n",
    "            self.layers.append(output_fc)\n",
    "            self.activations.append(Softmax())\n",
    "            self.n_layers = len(self.layers) # Update n_layers\n",
    "\n",
    "    def fit(self, X, y, n_iter=100, batch_size=32, verbose=True):\n",
    "        n_samples = X.shape[0]\n",
    "        self.loss_history = []\n",
    "\n",
    "        for i in range(n_iter):\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            X_shuffled = X[indices]\n",
    "            y_shuffled = y[indices]\n",
    "\n",
    "            for batch_start in range(0, n_samples, batch_size):\n",
    "                batch_end = batch_start + batch_size\n",
    "                X_batch = X_shuffled[batch_start:batch_end]\n",
    "                y_batch = y_shuffled[batch_start:batch_end]\n",
    "\n",
    "                forward_outputs = [X_batch]\n",
    "                Z = X_batch\n",
    "                for layer, activation in zip(self.layers, self.activations):\n",
    "                    A = layer.forward(Z)\n",
    "                    Z = activation.forward(A)\n",
    "                    forward_outputs.append(Z)\n",
    "\n",
    "                dA = self._loss_function_backward(forward_outputs[-1], y_batch)\n",
    "                for i_layer in range(len(self.layers) - 1, -1, -1):\n",
    "                    layer = self.layers[i_layer]\n",
    "                    activation = self.activations[i_layer]\n",
    "                    Z_current = forward_outputs[i_layer + 1]\n",
    "\n",
    "                    dZ_prev = layer.backward(dA)\n",
    "                    if isinstance(activation, Softmax):\n",
    "                        dA = activation.backward(Z_current, y_batch)\n",
    "                    else:\n",
    "                        dA = activation.backward(dZ_prev)\n",
    "\n",
    "            Z_epoch = self._forward(X)\n",
    "            loss = self._loss_function_forward(Z_epoch, y)\n",
    "            self.loss_history.append(loss)\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Epoch {i+1}/{n_iter}, Loss: {loss:.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        Z = self._forward(X)\n",
    "        return np.argmax(Z, axis=1)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self._forward(X)\n",
    "\n",
    "    def _forward(self, X):\n",
    "        Z = X\n",
    "        for layer, activation in zip(self.layers, self.activations):\n",
    "            A = layer.forward(Z)\n",
    "            Z = activation.forward(A)\n",
    "        return Z\n",
    "\n",
    "    def _loss_function_forward(self, Z, y):\n",
    "        batch_size = y.shape[0]\n",
    "        loss = -np.sum(np.log(Z[np.arange(batch_size), y] + 1e-7)) / batch_size\n",
    "        return loss\n",
    "\n",
    "    def _loss_function_backward(self, Z, y):\n",
    "        batch_size = y.shape[0]\n",
    "        dA = Z - np.eye(Z.shape[1])[y] / batch_size\n",
    "        return dA\n",
    "\n",
    "class FC:\n",
    "    def __init__(self, n_in, n_out, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self.W = initializer.W(n_in, n_out)\n",
    "        self.B = initializer.B(n_out)\n",
    "        self.X = None\n",
    "        self.dW = None\n",
    "        self.dB = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        self.A = np.dot(X, self.W) + self.B\n",
    "        return self.A\n",
    "\n",
    "    def backward(self, dA):\n",
    "        dZ = np.dot(dA, self.W.T)\n",
    "        self.dW = np.dot(self.X.T, dA)\n",
    "        self.dB = np.sum(dA, axis=0)\n",
    "        self.optimizer.update(self)\n",
    "        return dZ\n",
    "\n",
    "class SimpleInitializer:\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def W(self, n_in, n_out):\n",
    "        return np.random.normal(0, self.sigma, (n_in, n_out))\n",
    "\n",
    "    def B(self, n_out):\n",
    "        return np.zeros(n_out)\n",
    "\n",
    "class XavierInitializer:\n",
    "    def W(self, n_in, n_out):\n",
    "        sigma = 1.0 / np.sqrt(n_in)\n",
    "        W = np.random.normal(0, sigma, (n_in, n_out))\n",
    "        return W\n",
    "\n",
    "    def B(self, n_out):\n",
    "        B = np.zeros(n_out)\n",
    "        return B\n",
    "\n",
    "class HeInitializer:\n",
    "    def W(self, n_in, n_out):\n",
    "        sigma = np.sqrt(2.0 / n_in)\n",
    "        W = np.random.normal(0, sigma, (n_in, n_out))\n",
    "        return W\n",
    "\n",
    "    def B(self, n_out):\n",
    "        B = np.zeros(n_out)\n",
    "        return B\n",
    "\n",
    "class Tanh:\n",
    "    def forward(self, A):\n",
    "        self.Z = np.tanh(A)\n",
    "        return self.Z\n",
    "\n",
    "    def backward(self, dZ):\n",
    "        dA = dZ * (1 - self.Z**2)\n",
    "        return dA\n",
    "\n",
    "class ReLU:\n",
    "    def forward(self, A):\n",
    "        self.mask = (A > 0)\n",
    "        Z = np.maximum(0, A)\n",
    "        return Z\n",
    "\n",
    "    def backward(self, dZ):\n",
    "        dA = dZ * self.mask\n",
    "        return dA\n",
    "\n",
    "class Softmax:\n",
    "    def forward(self, A):\n",
    "        exp_A = np.exp(A - np.max(A, axis=1, keepdims=True))\n",
    "        self.Z = exp_A / np.sum(exp_A, axis=1, keepdims=True)\n",
    "        return self.Z\n",
    "\n",
    "    def backward(self, Z, y):\n",
    "        batch_size = y.shape[0]\n",
    "        dA = Z - np.eye(Z.shape[1])[y] / batch_size\n",
    "        return dA\n",
    "\n",
    "class SGD:\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, layer):\n",
    "        layer.W -= self.lr * layer.dW\n",
    "        layer.B -= self.lr * layer.dB\n",
    "\n",
    "class AdaGrad:\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        self.hw = None\n",
    "        self.hb = None\n",
    "        self.epsilon = 1e-7\n",
    "\n",
    "    def update(self, layer):\n",
    "        if self.hw is None:\n",
    "            self.hw = np.zeros_like(layer.W)\n",
    "            self.hb = np.zeros_like(layer.B)\n",
    "\n",
    "        self.hw += layer.dW * layer.dW\n",
    "        self.hb += layer.dB * layer.dB\n",
    "\n",
    "        layer.W -= self.lr * (1 / (np.sqrt(self.hw) + self.epsilon)) * layer.dW\n",
    "        layer.B -= self.lr * (1 / (np.sqrt(self.hb) + self.epsilon)) * layer.dB\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Example usage:\n",
    "    n_features = 784\n",
    "    n_output = 10\n",
    "    n_layers = 3\n",
    "\n",
    "    layer_configs = [\n",
    "        {'type': 'FC', 'n_nodes': 128, 'activation': 'ReLU', 'initializer': HeInitializer(), 'optimizer': AdaGrad(lr=0.01)},\n",
    "        {'type': 'FC', 'n_nodes': 64, 'activation': 'ReLU', 'initializer': HeInitializer(), 'optimizer': AdaGrad(lr=0.01)},\n",
    "        {'type': 'FC', 'n_nodes': n_output, 'activation': 'Softmax', 'initializer': XavierInitializer(), 'optimizer': AdaGrad(lr=0.01)}\n",
    "    ]\n",
    "\n",
    "    model = ScratchDeepNeuralNetrowkClassifier(n_layers, layer_configs, n_features, n_output, random_state=42)\n",
    "\n",
    "    # Dummy training data\n",
    "    X_train = np.random.rand(1000, n_features)\n",
    "    y_train = np.random.randint(0, n_output, 1000)\n",
    "\n",
    "    # Dummy test data\n",
    "    X_test = np.random.rand(200, n_features)\n",
    "    y_test = np.random.randint(0, n_output, 200)\n",
    "\n",
    "    #model.fit(X_train, y_train, n_iter=10, batch_size=32, verbose=True)\n",
    "\n",
    "    predictions = model.predict(X_test)\n",
    "    print(\"Predictions (first 10):\", predictions[:10])\n",
    "\n",
    "    probabilities = model.predict_proba(X_test)\n",
    "    print(\"Probabilities (first 2 samples):\\n\", probabilities[:2])\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d1b735",
   "metadata": {},
   "source": [
    "【Problem 9 】 Learning and estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4b08688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (44800, 784)\n",
      "y_train shape: (44800,)\n",
      "X_test shape: (14000, 784)\n",
      "y_test shape: (14000,)\n",
      "\n",
      "Training Network 1 (2 layers, ReLU):\n",
      "\n",
      "Training Network 2 (3 layers, ReLU):\n",
      "\n",
      "Training Network 3 (2 layers, Tanh):\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class ScratchDeepNeuralNetrowkClassifier:\n",
    "    \"\"\"\n",
    "    A versatile deep neural network classifier.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_layers, layer_configs, n_features, n_output, random_state=None):\n",
    "        self.n_layers = n_layers\n",
    "        self.layer_configs = layer_configs\n",
    "        self.n_features = n_features\n",
    "        self.n_output = n_output\n",
    "        self.layers = []\n",
    "        self.activations = []\n",
    "        self.loss_history = []\n",
    "        self.random_state = random_state\n",
    "        if self.random_state is not None:\n",
    "            np.random.seed(self.random_state)\n",
    "        self._build_network()\n",
    "\n",
    "    def _build_network(self):\n",
    "        input_dim = self.n_features\n",
    "        for i, config in enumerate(self.layer_configs):\n",
    "            layer_type = config['type']\n",
    "            n_nodes = config['n_nodes']\n",
    "            activation_type = config['activation']\n",
    "            initializer = config['initializer']\n",
    "            optimizer = config['optimizer']\n",
    "\n",
    "            if layer_type == 'FC':\n",
    "                fc_layer = FC(input_dim, n_nodes, initializer, optimizer)\n",
    "                self.layers.append(fc_layer)\n",
    "                if activation_type == 'Tanh':\n",
    "                    self.activations.append(Tanh())\n",
    "                elif activation_type == 'Softmax':\n",
    "                    self.activations.append(Softmax())\n",
    "                elif activation_type == 'ReLU':\n",
    "                    self.activations.append(ReLU())\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown activation type: {activation_type}\")\n",
    "                input_dim = n_nodes\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown layer type: {layer_type}\")\n",
    "\n",
    "        # Ensure the last layer has n_output nodes and Softmax activation\n",
    "        if not self.layer_configs or self.layer_configs[-1]['n_nodes'] != self.n_output or self.layer_configs[-1]['activation'] != 'Softmax':\n",
    "            last_input_dim = self.layer_configs[-1]['n_nodes'] if self.layer_configs else self.n_features\n",
    "            output_config = {'type': 'FC', 'n_nodes': self.n_output, 'activation': 'Softmax', 'initializer': XavierInitializer(), 'optimizer': AdaGrad(self.layer_configs[-1]['optimizer'].lr if self.layer_configs else 0.01)}\n",
    "            output_fc = FC(last_input_dim, self.n_output, output_config['initializer'], output_config['optimizer'])\n",
    "            self.layers.append(output_fc)\n",
    "            self.activations.append(Softmax())\n",
    "            self.n_layers = len(self.layers) # Update n_layers\n",
    "\n",
    "    def fit(self, X, y, n_iter=10, batch_size=64, verbose=True):\n",
    "        n_samples = X.shape[0]\n",
    "        self.loss_history = []\n",
    "\n",
    "        for i in range(n_iter):\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            X_shuffled = X[indices]\n",
    "            y_shuffled = y[indices]\n",
    "\n",
    "            for batch_start in range(0, n_samples, batch_size):\n",
    "                batch_end = batch_start + batch_size\n",
    "                X_batch = X_shuffled[batch_start:batch_end]\n",
    "                y_batch = y_shuffled[batch_start:batch_end]\n",
    "\n",
    "                forward_outputs = [X_batch]\n",
    "                Z = X_batch\n",
    "                for layer, activation in zip(self.layers, self.activations):\n",
    "                    A = layer.forward(Z)\n",
    "                    Z = activation.forward(A)\n",
    "                    forward_outputs.append(Z)\n",
    "\n",
    "                dA = self._loss_function_backward(forward_outputs[-1], y_batch)\n",
    "                for i_layer in range(len(self.layers) - 1, -1, -1):\n",
    "                    layer = self.layers[i_layer]\n",
    "                    activation = self.activations[i_layer]\n",
    "                    Z_current = forward_outputs[i_layer + 1]\n",
    "\n",
    "                    dZ_prev = layer.backward(dA)\n",
    "                    if isinstance(activation, Softmax):\n",
    "                        dA = activation.backward(Z_current, y_batch)\n",
    "                    else:\n",
    "                        dA = activation.backward(dZ_prev)\n",
    "\n",
    "            Z_epoch = self._forward(X)\n",
    "            loss = self._loss_function_forward(Z_epoch, y)\n",
    "            self.loss_history.append(loss)\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Epoch {i+1}/{n_iter}, Loss: {loss:.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        Z = self._forward(X)\n",
    "        return np.argmax(Z, axis=1)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self._forward(X)\n",
    "\n",
    "    def _forward(self, X):\n",
    "        Z = X\n",
    "        for layer, activation in zip(self.layers, self.activations):\n",
    "            A = layer.forward(Z)\n",
    "            Z = activation.forward(A)\n",
    "        return Z\n",
    "\n",
    "    def _loss_function_forward(self, Z, y):\n",
    "        batch_size = y.shape[0]\n",
    "        loss = -np.sum(np.log(Z[np.arange(batch_size), y] + 1e-7)) / batch_size\n",
    "        return loss\n",
    "\n",
    "    def _loss_function_backward(self, Z, y):\n",
    "        batch_size = y.shape[0]\n",
    "        dA = Z - np.eye(Z.shape[1])[y] / batch_size\n",
    "        return dA\n",
    "\n",
    "class FC:\n",
    "    def __init__(self, n_in, n_out, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self.W = initializer.W(n_in, n_out)\n",
    "        self.B = initializer.B(n_out)\n",
    "        self.X = None\n",
    "        self.dW = None\n",
    "        self.dB = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        self.A = np.dot(X, self.W) + self.B\n",
    "        return self.A\n",
    "\n",
    "    def backward(self, dA):\n",
    "        dZ = np.dot(dA, self.W.T)\n",
    "        self.dW = np.dot(self.X.T, dA)\n",
    "        self.dB = np.sum(dA, axis=0)\n",
    "        self.optimizer.update(self)\n",
    "        return dZ\n",
    "\n",
    "class SimpleInitializer:\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def W(self, n_in, n_out):\n",
    "        return np.random.normal(0, self.sigma, (n_in, n_out))\n",
    "\n",
    "    def B(self, n_out):\n",
    "        return np.zeros(n_out)\n",
    "\n",
    "class XavierInitializer:\n",
    "    def W(self, n_in, n_out):\n",
    "        sigma = 1.0 / np.sqrt(n_in)\n",
    "        W = np.random.normal(0, sigma, (n_in, n_out))\n",
    "        return W\n",
    "\n",
    "    def B(self, n_out):\n",
    "        B = np.zeros(n_out)\n",
    "        return B\n",
    "\n",
    "class HeInitializer:\n",
    "    def W(self, n_in, n_out):\n",
    "        sigma = np.sqrt(2.0 / n_in)\n",
    "        W = np.random.normal(0, sigma, (n_in, n_out))\n",
    "        return W\n",
    "\n",
    "    def B(self, n_out):\n",
    "        B = np.zeros(n_out)\n",
    "        return B\n",
    "\n",
    "class Tanh:\n",
    "    def forward(self, A):\n",
    "        self.Z = np.tanh(A)\n",
    "        return self.Z\n",
    "\n",
    "    def backward(self, dZ):\n",
    "        dA = dZ * (1 - self.Z**2)\n",
    "        return dA\n",
    "\n",
    "class ReLU:\n",
    "    def forward(self, A):\n",
    "        self.mask = (A > 0)\n",
    "        Z = np.maximum(0, A)\n",
    "        return Z\n",
    "\n",
    "    def backward(self, dZ):\n",
    "        dA = dZ * self.mask\n",
    "        return dA\n",
    "\n",
    "class Softmax:\n",
    "    def forward(self, A):\n",
    "        exp_A = np.exp(A - np.max(A, axis=1, keepdims=True))\n",
    "        self.Z = exp_A / np.sum(exp_A, axis=1, keepdims=True)\n",
    "        return self.Z\n",
    "\n",
    "    def backward(self, Z, y):\n",
    "        batch_size = y.shape[0]\n",
    "        dA = Z - np.eye(Z.shape[1])[y] / batch_size\n",
    "        return dA\n",
    "\n",
    "class SGD:\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, layer):\n",
    "        layer.W -= self.lr * layer.dW\n",
    "        layer.B -= self.lr * layer.dB\n",
    "\n",
    "class AdaGrad:\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        self.hw = None\n",
    "        self.hb = None\n",
    "        self.epsilon = 1e-7\n",
    "\n",
    "    def update(self, layer):\n",
    "        if self.hw is None:\n",
    "            self.hw = np.zeros_like(layer.W)\n",
    "            self.hb = np.zeros_like(layer.B)\n",
    "\n",
    "        self.hw += layer.dW * layer.dW\n",
    "        self.hb += layer.dB * layer.dB\n",
    "\n",
    "        layer.W -= self.lr * (1 / (np.sqrt(self.hw) + self.epsilon)) * layer.dW\n",
    "        layer.B -= self.lr * (1 / (np.sqrt(self.hb) + self.epsilon)) * layer.dB\n",
    "\n",
    "# Load MNIST data\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "X = mnist.data.astype(np.float32)\n",
    "y = mnist.target.astype(np.int64)\n",
    "\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "X /= 255\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Further split training data for validation (optional)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "n_features = X_train.shape[1]\n",
    "n_output = len(np.unique(y))\n",
    "\n",
    "def train_and_evaluate(layer_configs, n_features, n_output, X_train, y_train, X_test, y_test, n_iter=10, batch_size=64, random_state=42, verbose=True):\n",
    "    \"\"\"\n",
    "    Creates, trains, and evaluates a neural network with the given configuration.\n",
    "    \"\"\"\n",
    "    model = ScratchDeepNeuralNetrowkClassifier(\n",
    "        n_layers=len(layer_configs) + 1,  # +1 for the input layer\n",
    "        layer_configs=layer_configs,\n",
    "        n_features=n_features,\n",
    "        n_output=n_output,\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train, n_iter=n_iter, batch_size=batch_size, verbose=verbose)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    plt.plot(model.loss_history)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training Loss\")\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    return accuracy, model.loss_history\n",
    "\n",
    "# --- Problem 9 ---\n",
    "\n",
    "# Network 1: Simple Network (2 layers, ReLU)\n",
    "layer_config_1 = [\n",
    "    {'type': 'FC', 'n_nodes': 128, 'activation': 'ReLU', 'initializer': HeInitializer(), 'optimizer': AdaGrad(lr=0.01)},\n",
    "    {'type': 'FC', 'n_nodes': n_output, 'activation': 'Softmax', 'initializer': XavierInitializer(), 'optimizer': AdaGrad(lr=0.01)}\n",
    "]\n",
    "print(\"\\nTraining Network 1 (2 layers, ReLU):\")\n",
    "#accuracy_1, loss_history_1 = train_and_evaluate(layer_config_1, n_features, n_output, X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Network 2: Deeper Network (3 layers, ReLU)\n",
    "layer_config_2 = [\n",
    "    {'type': 'FC', 'n_nodes': 256, 'activation': 'ReLU', 'initializer': HeInitializer(), 'optimizer': AdaGrad(lr=0.01)},\n",
    "    {'type': 'FC', 'n_nodes': 128, 'activation': 'ReLU', 'initializer': HeInitializer(), 'optimizer': AdaGrad(lr=0.01)},\n",
    "    {'type': 'FC', 'n_nodes': n_output, 'activation': 'Softmax', 'initializer': XavierInitializer(), 'optimizer': AdaGrad(lr=0.01)}\n",
    "]\n",
    "print(\"\\nTraining Network 2 (3 layers, ReLU):\")\n",
    "#accuracy_2, loss_history_2 = train_and_evaluate(layer_config_2, n_features, n_output, X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Network 3: Simple Network (2 layers, Tanh)\n",
    "layer_config_3 = [\n",
    "    {'type': 'FC', 'n_nodes': 128, 'activation': 'Tanh', 'initializer': XavierInitializer(), 'optimizer': AdaGrad(lr=0.01)},\n",
    "    {'type': 'FC', 'n_nodes': n_output, 'activation': 'Softmax', 'initializer': XavierInitializer(), 'optimizer': AdaGrad(lr=0.01)}\n",
    "]\n",
    "print(\"\\nTraining Network 3 (2 layers, Tanh):\")\n",
    "#accuracy_3, loss_history_3 = train_and_evaluate(layer_config_3, n_features, n_output, X_train, y_train, X_test, y_test)\n",
    "\n",
    "# --- End of Problem 9 ---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
